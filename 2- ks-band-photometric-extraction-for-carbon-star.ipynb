{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n\nThis notebook is a **continuation** of the previous work titled [*Catalog-Based Positional Matching of Carbon Stars*](https://www.kaggle.com/code/nicolsottero/catalog-based-positional-matching-of-carbon-stars), where we identified the closest infrared sources to five known carbon stars across 48 deep-sky catalogs.\n\nIn this second stage, we demonstrate the full analysis pipeline **using carbon star `s1` as a representative example**. The same methodology can later be applied to the other stars (`s0`, `s2`, `s3`, `s4`).\n\nThe goal is to:\n\n- Build an **infrared light curve** using `Ks`-band magnitudes\n- Perform data cleaning and imputation\n- Apply machine learning to predict future brightness values\n\nThe full pipeline integrates:\n\n- Renaming and organizing raw magnitude files  \n- Merging them with observation metadata from `lista.cat`  \n- Filtering for `Ks`-band data only  \n- Interpolating and smoothing the light curve  \n- Filling gaps with a **Random Forest Regressor**  \n- Forecasting with a **Long Short-Term Memory (LSTM)** neural network\n\nThis project bridges traditional catalog-based analysis with modern machine learning to study long-term variability in evolved stars like carbon stars.\n","metadata":{}},{"cell_type":"markdown","source":"### Data Recap: Source Files and Metadata\n\nAs a reminder, the dataset used in this project was kindly provided by **PhD. David Merlo** and includes the following components:\n\n- **`Merlo2015.pdf`**  \n  A scientific article that provides the theoretical and observational context for the study, including previous work on carbon stars and infrared variability.\n\n- **Multiple `.asc` files**  \n  Raw catalogs of infrared sources, each corresponding to a different photometric filter (Ks, H, J, Y, Z). These files are sorted by celestial coordinates and contain magnitudes, errors, and source classification labels.\n\n- **`lista.cat`**  \n  A metadata file summarizing the content of each `.asc` file, including the observation date, filter used, and file name reference.\n\n- **`fuentes.txt`**  \n  A list of **five carbon stars**, labeled `s0` to `s4`, with their precise **J2000 equatorial coordinates**. These stars serve as the reference targets for matching and time-series analysis. Star `s1` corresponds to the coordinates listed under label `s1` in `fuentes.txt`.\n\n- **Additional test files** (e.g., `Ks20s0`, `Ks20s1`, etc.)  \n  Used for preliminary inspection and early-stage validation of magnitudes across epochs.\n\nThis notebook builds upon these data components to generate and model infrared light curves for the selected stars.\n","metadata":{}},{"cell_type":"markdown","source":"### üîÅ Recap from the Previous Notebook: Generating `_results.csv` Files\n\nEach raw catalog file was originally named using the following pattern:\n\n**vYYYYMMDD_NNNNN_st_tl_cat.asc**\n","metadata":{}},{"cell_type":"markdown","source":"\nFor example:  \n`v20100313_00394_st_tl_cat.asc`\n\nThese `.asc` files contain catalogs of thousands of sources detected in infrared observations.  \nThe objective of the previous notebook ([*Catalog-Based Positional Matching of Carbon Stars*](https://www.kaggle.com/code/nicolsottero/catalog-based-positional-matching-of-carbon-stars)) was to extract only the closest stellar source (type `-1`) to each of the five reference carbon stars (`s0` to `s4`) from each `.asc` file.\n\n**To facilitate future time series construction**, only the filtered brightness measurements of these 5 targets were retained.  \nEach result was saved as a new CSV using the same base name plus `_results.csv`.  \nFor example:\n\n- `v20100313_00394_st_tl_cat.asc` ‚Üí `v20100313_00394_st_tl_cat_results.csv`\n\nThis ensured:\n- Direct traceability between raw catalogs and result files.\n- Simplified handling for downstream analysis.\n\n---\n\n#### üß© Code Excerpt\n\nThe following block, shown here a reference, was used to save the filtered results:\n\n```python\n                # Save results to CSV\n                if closest_stars:\n                    output_filename = f\"{os.path.splitext(filename)[0]}_results.csv\"\n                    df_closest = pd.DataFrame(closest_stars)\n                    df_closest.to_csv(output_filename, mode='w', header=True, index=False)\n                    files.download(output_filename)\n```\n\nThese _results.csv files now serve as the foundation for constructing light curves across time and filters in the current notebook.","metadata":{}},{"cell_type":"markdown","source":"## 1. Extracting Magnitudes for Light Curve Construction\n\nWe now begin the first proper stage of this notebook. The previous section served as a general introduction.\n\nIn this step, we obtain the file **`merged_result_s1.csv`**, which will be used for the upcoming light curve analysis.\n\nThis file is built by collecting the magnitude values of the **s1** carbon star across all catalogs. To do this, we rely on the individual **`_results.csv`** files generated in the [previous notebook](https://www.kaggle.com/code/nicolsottero/catalog-based-positional-matching-of-carbon-stars), where each file contains only the five closest stellar sources (type = -1) to the carbon stars s0 through s4.\n\nThese result files are renamed to match the identifiers listed in the **`lista.cat`** file, which serves as a catalog index.\n\n### What is `lista.cat`?\n\nThis file associates each original `.asc` file with key metadata:\n- The photometric filter used (Ks, H, J, Y, Z)\n- The observation date\n- The original filename\n\nBy renaming each `_results.csv` file accordingly, we ensure compatibility with `lista.cat`, allowing us to seamlessly merge brightness measurements with their corresponding filters and timestamps. This enables the construction of accurate and well-labeled time series for each carbon star.\n\n### Extracting Magnitudes for s1 Across All Catalogs\n\nThe following code loads the `*_results.csv` files generated in the previous notebook.  \nEach file contains photometric measurements for five carbon stars extracted from one catalog.\n\nThe process involves:\n- Uploading these CSVs (renamed to match their original `.asc` catalog filenames).\n- Parsing the SkyCoord string column to ensure valid coordinate format.\n- Filtering each file to find the closest stellar source to star **s1** based on angular distance.\n- Extracting its magnitude and error.\n\nThe output is a file named `final_result_s1.csv`, listing the brightness of s1 across all catalogs.  \nThis data will be used to build its light curve.\n","metadata":{}},{"cell_type":"code","source":"# Execution guard to prevent running this cell accidentally\nrun_block = False\n\nif run_block:\n\n    from google.colab import files\n    import pandas as pd\n    import io\n    from astropy.coordinates import SkyCoord\n    import astropy.units as u\n\n    # Ensure coordinate strings are parsed correctly\n    def parse_skycoord(coord_str):\n        try:\n            coord_str = coord_str.replace('<SkyCoord (ICRS): (ra, dec) in deg', '').strip()\n            coord_str = coord_str.replace('>', '').replace('(', '').replace(')', '').strip()\n            ra_dec_list = coord_str.split(',')\n            if len(ra_dec_list) == 2:\n                ra = float(ra_dec_list[0].strip())\n                dec = float(ra_dec_list[1].strip())\n                return SkyCoord(ra=ra*u.deg, dec=dec*u.deg, frame='icrs')\n            else:\n                raise ValueError(\"Invalid coordinate format\")\n        except Exception as e:\n            print(f\"Error parsing coord: {coord_str} -> {e}\")\n            return None\n\n    def load_catalogs_and_find_star_s1(uploaded_files, star_coords):\n        star_data = []\n        for file_name, file_content in uploaded_files.items():\n            df = pd.read_csv(io.BytesIO(file_content), header=None)\n            df.columns = ['RA_h', 'RA_m', 'RA_s', 'Dec_g', 'Dec_m', 'Dec_s',\n                          'Mag', 'ErrorMag', 'Fuente', 'coord', 'separation', 'carbon_star_key']\n            df = df.drop(0)  # Remove extra header row\n            df['coord'] = df['coord'].apply(parse_skycoord)\n            df = df.dropna(subset=['coord'])\n            df['separation'] = df['coord'].apply(lambda x: x.separation(star_coords).arcsecond)\n            closest_star = df.loc[df['separation'].idxmin()]\n            star_data.append({\n                'file_name': file_name,\n                'magnitude': closest_star['Mag'],\n                'magnitude_error': closest_star['ErrorMag']\n            })\n        return star_data\n\n    s1_coords = SkyCoord(ra=183.99373333*u.deg, dec=-64.34371111*u.deg, frame='icrs')\n\n    uploaded_files = files.upload()\n\n    s1_data = load_catalogs_and_find_star_s1(uploaded_files, s1_coords)\n\n    output_df = pd.DataFrame(s1_data)\n    print(\"List of s1 magnitudes across different catalogs:\")\n    print(output_df[['file_name', 'magnitude', 'magnitude_error']])\n\n    output_df.to_csv('/content/final_result_s1.csv', index=False)\n    files.download('/content/final_result_s1.csv')\n\nelse:\n    print(\"Execution disabled\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Merging Metadata and Magnitudes for Ks Band\n\nThis section focuses on creating a clean, unified table (`merged_result_s1.csv`) that combines:\n\n- The Ks-band metadata for each catalog (from `lista.cat`)\n- The measured magnitudes of star **s1** from the `_results.csv` files\n\nThe process was carried out in three steps:\n\n---\n\n#### Filtering Ks Band Entries from `lista.cat`\n\nThe original `lista.cat` file contains metadata for all catalogs (filter band, observation date, etc.).  \nA cleaned version named `CSV_lista.txt` was uploaded and processed to retain only the entries corresponding to the **Ks** filter.\n\n- The file was loaded using pandas\n- Rows with band `\"Ks\"` were selected\n- The result was saved as `Ks_catalogs_s1.csv` for future use\n\n---\n\n#### Cleaning `file_name` Columns in Both Tables\n\nTo ensure a successful merge, the column `file_name` from both `Ks_catalogs_s1.csv` and `final_result_s1.csv` was sanitized.\n\n- The file extensions `.csv` and `.asc` were stripped from all values\n- This standardization allows the two datasets to match on catalog names\n- The cleaned files were saved as `Ks_catalogs_s1_modified.csv` and `final_result_s1_modified.csv`\n\n---\n\n#### Merging Metadata and Photometry into a Single Table\n\nThe cleaned metadata and photometric measurements were merged based on the `file_name` column using an **outer join**.\n\n- The outer join ensures no data is lost if a match is missing on either side\n- The final result, `merged_result_s1.csv`, contains:\n  - Observation metadata (e.g. filter, MJD, exposure time)\n  - Photometric magnitude and error for star **s1** in each catalog\n\nThis consolidated table is now ready for time-series analysis and curve fitting.\n","metadata":{}},{"cell_type":"markdown","source":"### 2.1. Filtering Ks Band Entries from `lista.cat`\n\nThe file `CSV_lista.txt` is a cleaned version of the original `lista.cat`, which lists metadata for each catalog file, including its filter band, observation date (MJD), and exposure time.\n\nIn this step, we filter the entries to keep only those corresponding to the **Ks band**, since this wavelength is of particular interest for the photometric analysis of carbon star **s1**.\n\nThe result is saved as `Ks_catalogs_s1.csv` for later merging with photometric measurements.\n\n> Below is the code used to upload and process the file. Execution has been disabled to preserve reproducibility:\n","metadata":{}},{"cell_type":"code","source":"# Code block to upload and filter CSV_lista.txt ‚Äì execution disabled\nrun_filter = False\n\nif run_filter:\n    from google.colab import files\n    import pandas as pd\n\n    # Upload the CSV_lista.txt file\n    uploaded = files.upload()\n\n    # Read the CSV file\n    csv_lista_path = 'CSV_lista.txt'\n    lista_cat_df = pd.read_csv(csv_lista_path)\n\n    # Filter for entries with the Ks band\n    Ks_catalogs_s1_df = lista_cat_df[lista_cat_df['Banda'] == 'Ks']\n\n    # Save the result for later use\n    Ks_catalogs_s1_df.to_csv('Ks_catalogs_s1.csv', index=False)\n\n    # Download the result\n    files.download('Ks_catalogs_s1.csv')\nelse:\n    print(\"Execution disabled\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.2. Cleaning `file_name` Columns\n\nTo ensure a proper merge between photometric measurements and catalog metadata, we removed extensions such as `.csv` and `.asc` from the `file_name` columns in both datasets:\n\n- `Ks_catalogs_s1.csv` (catalog metadata with band = Ks)\n- `final_result_s1.csv` (magnitudes extracted for carbon star s1)\n\nThis normalization avoids mismatches caused by filename formatting and prepares the data for merging.\n\n> Below is the code used to perform the cleaning. Execution is disabled for safety:\n","metadata":{}},{"cell_type":"code","source":"# Code block to clean file_name columns ‚Äì execution disabled\nrun_cleaning = False\n\nif run_cleaning:\n    from google.colab import files\n    import pandas as pd\n\n    # Upload the modified Ks catalog and magnitude result files\n    uploaded = files.upload()\n\n    # Extract paths\n    Ks_catalogs_s1_path = [path for path in uploaded.keys() if \"Ks_catalogs_s1\" in path][0]\n    final_result_s1_path = [path for path in uploaded.keys() if \"final_result_s1\" in path][0]\n\n    # Load the dataframes\n    Ks_catalogs_s1_df = pd.read_csv(Ks_catalogs_s1_path)\n    final_result_s1_df = pd.read_csv(final_result_s1_path)\n\n    # Remove file extensions from 'file_name' columns\n    Ks_catalogs_s1_df['file_name'] = Ks_catalogs_s1_df['file_name'].str.replace('.csv', '', regex=False)\n    Ks_catalogs_s1_df['file_name'] = Ks_catalogs_s1_df['file_name'].str.replace('.asc', '', regex=False)\n    final_result_s1_df['file_name'] = final_result_s1_df['file_name'].str.replace('.csv', '', regex=False)\n    final_result_s1_df['file_name'] = final_result_s1_df['file_name'].str.replace('.asc', '', regex=False)\n\n    # Save and download the cleaned files\n    Ks_catalogs_s1_df.to_csv('Ks_catalogs_s1_modified.csv', index=False)\n    final_result_s1_df.to_csv('final_result_s1_modified.csv', index=False)\n    files.download('Ks_catalogs_s1_modified.csv')\n    files.download('final_result_s1_modified.csv')\nelse:\n    print(\"Execution disabled for reproducibility.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2.3. Merging Catalog Metadata with Photometric Results\n\nFinally, both cleaned datasets were merged to produce a unified table:\n\n- `Ks_catalogs_s1_modified.csv`: metadata from `lista.cat` filtered for the Ks band.\n- `final_result_s1_modified.csv`: magnitude and error for carbon star s1 across all catalogs.\n\nThe merge was performed on the `file_name` column after extension cleanup.\n\nThis operation yielded a complete dataset named `merged_result_s1.csv`, ready for time series analysis and visualization.\n\n> Code block used for merging (execution disabled for reproducibility):\n","metadata":{}},{"cell_type":"code","source":"# Code to merge cleaned metadata and magnitude results ‚Äì execution disabled\nrun_merge = False\n\nif run_merge:\n    from google.colab import files\n    import pandas as pd\n\n    # Upload cleaned metadata and results\n    uploaded = files.upload()\n\n    # Extract file paths\n    ks_catalogs_s1_filename = [path for path in uploaded.keys() if \"Ks_catalogs_s1_modified\" in path][0]\n    final_result_s1_filename = [path for path in uploaded.keys() if \"final_result_s1_modified\" in path][0]\n\n    # Load data\n    ks_catalogs_s1_df = pd.read_csv(ks_catalogs_s1_filename)\n    final_result_s1_df = pd.read_csv(final_result_s1_filename)\n\n    # Merge on 'file_name'\n    merged_s1_df = pd.merge(ks_catalogs_s1_df, final_result_s1_df, on='file_name', how='outer')\n\n    # Save merged results\n    merged_s1_df.to_csv('merged_result_s1.csv', index=False)\n    files.download('merged_result_s1.csv')\n\n    # Display output\n    print(\"Merged DataFrame:\")\n    print(merged_s1_df)\nelse:\n    print(\"Execution disabled for reproducibility.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Conclusion\n\nThis notebook successfully prepares the dataset required for photometric analysis and light curve modeling of the carbon star **s1** in the **Ks band**.\n\nStarting from the original `.asc` catalogs and the metadata in `lista.cat`, we:\n\n- Extracted the closest source to s1 in each catalog using positional matching.\n- Filtered the catalogs by Ks filter using a cleaned version of `lista.cat`.\n- Matched and merged the magnitude data with catalog metadata.\n\nThe final output, `merged_result_s1.csv`, contains for each Ks-band observation:\n\n- The corresponding photometric magnitude and error for s1.\n- The associated catalog metadata, including date and exposure information.\n\n***This file is now ready for time-series modeling and predictive analysis in the next phase.***\n","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}